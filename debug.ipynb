{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.nlp import TextDataset\n",
    "from transformers import AutoTokenizer\n",
    "from toolkit.enums import Split\n",
    "from toolkit.nlp import NLPTrainingConfig\n",
    "from load_data_fn import load_data_fn\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"pretrained_models/baichuan2-13b-chat\", trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Adding pad token {DEFAULT_PAD_TOKEN}\")\n",
    "    tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n",
    "\n",
    "train_dataset = TextDataset.from_file(\n",
    "    \"data/hot_finetune_data/train.json\",\n",
    "    tokenizer,\n",
    "    split=Split.TRAINING,\n",
    "    configs=NLPTrainingConfig(train_batch_size=64),\n",
    "    load_data_fn=load_data_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tokenizer.decode(train_dataset[0]['model_input']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0]['labels'])\n",
    "print(tokenizer.decode(abs(train_dataset[0]['labels']), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"你好呀</s>\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(-2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import deepspeed\n",
    "import hjson\n",
    "import numpy as np\n",
    "import toolkit\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from fire import Fire\n",
    "from toolkit import getLogger\n",
    "from toolkit.enums import Split\n",
    "from toolkit.metric import MetricDict\n",
    "from toolkit.nlp import TextDataset\n",
    "from toolkit.training import Trainer, initialize\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizer,\n",
    "    CONFIG_MAPPING,\n",
    ")\n",
    "from myconfig import MyTrainingConfig\n",
    "from load_data_fn import load_data_fn\n",
    "from toolkit.training.dataloader import get_dataloader\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "local_rank=0\n",
    "logger = getLogger(__name__, \"/dev/null\")\n",
    "config = MyTrainingConfig(parallel_mode=\"deepspeed\" ,\n",
    "    deepspeed_config =\"./ds_zero3_offload_mod.hjson\" ,\n",
    "    dashboard =\"tensorboard\" ,\n",
    "    model_dir =\"./baichuan-13b-chat\" ,\n",
    "    train_file_path =\"./data/hot_finetune_data/train.json\" ,\n",
    "    train_batch_size =8 ,\n",
    "    gradient_accumulation_steps =1 ,\n",
    "    seed =0 ,\n",
    "    fp16 =True ,\n",
    "    epochs =16 ,\n",
    "    opt_lr =\"1e-4\" ,\n",
    "    sch_warmup_ratio_steps =0.03 ,\n",
    "    opt_weight_decay =0 ,\n",
    "    ddp_timeout =30000 ,\n",
    "    torch_dtype =\"float16\" ,\n",
    "    logging_steps =1 ,\n",
    "    padding_side =\"left\" ,)\n",
    "\n",
    "def load_tokenizer() -> PreTrainedTokenizer:\n",
    "    # * Load tokenizer\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": config.cache_dir,\n",
    "        \"use_fast\": config.use_fast_tokenizer,\n",
    "        \"revision\": config.model_revision,\n",
    "        \"use_auth_token\": True if config.use_auth_token else None,\n",
    "    }\n",
    "    if config.model_dir:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_dir, **tokenizer_kwargs, trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "    # * resize embedding\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(f\"Adding pad token {DEFAULT_PAD_TOKEN}\")\n",
    "        tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n",
    "    logger.info(f\"len(tokenizer):{len(tokenizer)}\")\n",
    "    if dist.is_initialized():\n",
    "        dist.barrier()\n",
    "    return tokenizer\n",
    "\n",
    "def load_dataset(tokenizer: PreTrainedTokenizer) -> tuple:\n",
    "    # * Load training data, development data and test data\n",
    "    train_dataset = TextDataset.from_file(\n",
    "        config.train_file_path,\n",
    "        tokenizer,\n",
    "        split=Split.TRAINING,\n",
    "        configs=config,\n",
    "        load_data_fn=load_data_fn,\n",
    "    )\n",
    "    try:\n",
    "        val_dataset = TextDataset.from_file(\n",
    "            config.val_file_path,\n",
    "            tokenizer,\n",
    "            split=Split.VALIDATION,\n",
    "            configs=config,\n",
    "            load_data_fn=load_data_fn,\n",
    "        )\n",
    "    except TypeError as e:\n",
    "        if local_rank == 0:\n",
    "            logger.warning(e)\n",
    "        val_dataset = None\n",
    "    try:\n",
    "        test_dataset = TextDataset.from_file(\n",
    "            config.test_file_path,\n",
    "            tokenizer,\n",
    "            split=Split.TEST,\n",
    "            configs=config,\n",
    "            load_data_fn=load_data_fn,\n",
    "        )\n",
    "    except TypeError as e:\n",
    "        if local_rank == 0:\n",
    "            logger.warning(e)\n",
    "        test_dataset = None\n",
    "    if dist.is_initialized():\n",
    "        dist.barrier()\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def load_model(tokenizer) -> deepspeed.DeepSpeedEngine:\n",
    "    start = time.time()\n",
    "    # * Load model config\n",
    "    model_kwargs = {\n",
    "        \"cache_dir\": config.cache_dir,\n",
    "        \"revision\": config.model_revision,\n",
    "        \"use_auth_token\": True if config.use_auth_token else None,\n",
    "    }\n",
    "    if config.model_dir:\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            config.model_dir, **model_kwargs, trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        model_config = CONFIG_MAPPING[config.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "        if config.config_overrides is not None:\n",
    "            logger.info(f\"Overriding config: {config.config_overrides}\")\n",
    "            model_config.update_from_string(config.config_overrides)\n",
    "            logger.info(f\"New config: {config}\")\n",
    "    # * Load model\n",
    "    logger.debug(f\"local_rank {local_rank}: Loading model ...\")\n",
    "    if config.model_dir:\n",
    "        torch_dtype = (\n",
    "            config.torch_dtype\n",
    "            if config.torch_dtype in [\"auto\", None]\n",
    "            else getattr(torch, config.torch_dtype)\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_dir,\n",
    "            from_tf=bool(\".ckpt\" in config.model_dir),\n",
    "            config=model_config,\n",
    "            cache_dir=config.cache_dir,\n",
    "            revision=config.model_revision,\n",
    "            use_auth_token=True if config.use_auth_token else None,\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=False,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "        logger.info(\n",
    "            f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\"\n",
    "        )\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) != embedding_size:\n",
    "        logger.info(\"resize the embedding size by the size of the tokenizer\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    ds_model=model\n",
    "    # if config.parallel_mode == \"deepspeed\":\n",
    "    #     deepspeed_config = hjson.load(open(config.deepspeed_config, \"r\"))\n",
    "    #     config.set_deepspeed(deepspeed_config)\n",
    "    #     ds_model, _, _, _ = deepspeed.initialize(model=model, config=deepspeed_config)\n",
    "    end = time.time()\n",
    "    logger.debug(f\"local_rank {local_rank}: Loading model takes {end - start:.2f} sec.\")\n",
    "    return ds_model\n",
    "\n",
    "# * Loading tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "\n",
    "# * load dataset\n",
    "dataset_train, val_dataset, test_dataset = load_dataset(tokenizer)\n",
    "\n",
    "# *load model\n",
    "model = load_model(tokenizer)\n",
    "\n",
    "dataloader_train, sampler = get_dataloader(\n",
    "            dataset_train, config, Split.TRAINING, collate_fn=dataset_train.collate_fn, shuffle=config.shuffle\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader_train:\n",
    "    output = model(**batch, max_new_tokens=20)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/codes/train_llms/ngram.py:31: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  EN_PART_RE = re.compile(\"[\\s\\u0021-\\u007f]+\")\n",
      "WARNING:root:building dataset...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from build_dataset import MyDataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"pretrained_models/baichuan2-13b-chat\", trust_remote_code=True)\n",
    "path = Path(\"data/hot_finetune_data/\")\n",
    "files = [os.path.join(path,file.name) for file in path.glob(\"*.json\")]\n",
    "dataset = MyDataset(files, tokenizer, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.training import get_dataloader\n",
    "from toolkit.enums import Split\n",
    "from toolkit.nlp import NLPTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, _ = get_dataloader(dataset, NLPTrainingConfig(train_batch_size=8), Split.TRAINING, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed import DeepSpeedConfig\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"train_batch_size\":2\n",
    "}\n",
    "config = DeepSpeedConfig(ds_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_json('data/hot_finetune_data/train_v6.json', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = df.sample(100, replace=False)\n",
    "train = df.drop(dev.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100 entries, 127 to 5129\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  100 non-null    object\n",
      " 1   input        100 non-null    object\n",
      " 2   output       100 non-null    object\n",
      " 3   query        71 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5962 entries, 0 to 6061\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  5962 non-null   object\n",
      " 1   input        5962 non-null   object\n",
      " 2   output       5962 non-null   object\n",
      " 3   query        3890 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 232.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6062 entries, 0 to 6061\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  6062 non-null   object\n",
      " 1   input        6062 non-null   object\n",
      " 2   output       6062 non-null   object\n",
      " 3   query        3961 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 189.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dev.info()\n",
    "train.info()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'data/hot_finetune_data'\n",
    "from pathlib import Path\n",
    "\n",
    "d = Path(d)\n",
    "\n",
    "train_dir = d/'train'\n",
    "dev_dir = d/'dev'\n",
    "\n",
    "train_dir.mkdir(exist_ok=True)\n",
    "dev_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train.to_json(train_dir/\"all.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "dev.to_json(dev_dir/\"all.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125696\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "import torch\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"./pretrained_models/baichuan2-13b-chat/\", trust_remote_code=True)\n",
    "print(config.vocab_size)\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "from_pretrained_kwargs = dict(\n",
    "    from_tf=False,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=False,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "94*5*0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b86acde8f1383f86f3021849fe9adf6a3ab376df7b33a306f998570f99994f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
